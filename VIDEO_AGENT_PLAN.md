# Autonomous AI Video Production Agent: Implementation Plan

> **Generated by**: Agent team (code audit + video analysis + web research)
> **Date**: 2026-02-11
> **Status**: Ready for Claude Code implementation

---

## Executive Summary

Build an autonomous "faceless YouTube video" production agent on top of the existing **stockpile** project. The system takes a topic prompt and produces a complete video: script, narration, B-roll, subtitles, effects, and background music - with an AI Director (Gemini) reviewing drafts and requesting refinements.

**Key Insight**: Stockpile already has **~75-80%** of the infrastructure needed. The original Gemini plan massively underestimates what's already built and recommends replacing existing, battle-tested components with inferior alternatives. The only significant new module is the **video compositor** (MoviePy/FFmpeg assembly) - everything else is wiring existing services together.

---

## What Already Exists (Don't Rebuild)

### Reuse As-Is

| Component | Location | What It Does |
|-----------|----------|-------------|
| **TTS Service** (960 lines) | `src/services/tts_service.py` | **5 backends**: Chatterbox (Colab), RunPod Chatterbox (voice cloning), Public Chatterbox Turbo, Qwen3-TTS (preset speakers like "Ethan"/"Chelsie", style instructions like "Speak with confidence"), Chatterbox Extended (multi-candidate, denoising, Whisper validation). Smart chunking, WAV/MP3 merging, timeout recovery with recursive splitting. |
| **Voice Library** (415 lines) | `src/services/voice_library.py` | Persistent voice reference storage. Turso cloud DB or local filesystem. CRUD operations, favorites, audio bytes retrieval. |
| **AI Service** (1887 lines) | `src/services/ai_service.py` | Gemini 3 Flash with structured JSON output via `response_schema`. Content style detection, context question generation, best-image selection, bulk image prompt generation. AI response caching (100% cost savings on re-runs). Retry + exponential backoff. Prompt versioning in `src/services/prompts/`. |
| **Image Generation** (660 lines) | `src/services/image_generation_service.py` | **3 providers**: Runware (Flux Klein 4B at $0.0006/img, Flux Klein 9B), Gemini 2.5 Flash (**FREE** 500/day, supports aspect ratios), Nano Banana Pro (image-to-image editing/inpainting). Unified routing by model enum. |
| **Music Service** (192 lines) | `src/services/music_service.py` | **Stable Audio 2.5** via Replicate API. Generate up to 190 seconds of music by genre/mood. Async poll-based job tracking. |
| **Clip Extractor** (1425 lines) | `src/services/clip_extractor.py` | Gemini video analysis + FFmpeg extraction. Upload video to Gemini File API, get structured JSON timestamps + scores. Competitive multi-video analysis. CLIP visual matching. Scene detection (PySceneDetect). |
| **Video Sources** | `src/services/video_sources/` | **3 pluggable sources**: `PexelsVideoSource` (CC0 stock, async), `PixabayVideoSource`, `YouTubeVideoSource` (yt-dlp). All implement `VideoSource` interface with `search_videos()`. |
| **Image Sources** | `src/services/image_sources/` | **3 pluggable sources**: `PexelsImageSource`, `PixabayImageSource`, `GoogleImageSource` (SerpAPI). |
| **Image Acquisition** (368 lines) | `src/services/image_downloader.py` | `ImageAcquisitionService` - Full pipeline: search all sources → AI picks best → downloads winner. Content style awareness, parallel downloading with semaphore control. |
| **Video Downloader** | `src/services/video_downloader.py` | yt-dlp wrapper + `ParallelDownloader`. Rate limiting, user agent rotation, format fallbacks, two-pass download (360p preview → 1080p clip). Direct HTTP for Pexels/Pixabay. |
| **Transcription** (293 lines) | `src/services/transcription.py` | Dual backend: `faster-whisper` (CTranslate2, 4x faster) or `openai-whisper`. Returns segments with start/end timestamps + duration + language. Handles video and audio files. |
| **File Organizer** | `src/services/file_organizer.py` | Output folder structure management. |
| **Config System** | `src/utils/config.py` | .env loader with 50+ settings, validation. |
| **Retry Utilities** | `src/utils/retry.py` | `@retry_api_call` decorator. Custom error types: `APIRateLimitError`, `NetworkError`, `YouTubeRateLimitError`. |
| **Cost Tracker** | `src/utils/cost_tracker.py` | API cost tracking across all services. |
| **Checkpoint/Resume** | `src/utils/checkpoint.py` | Save/restore long-running job progress. |
| **Progress Tracking** | `src/utils/progress.py` | Processing status tracking. |
| **Web API** | `src/api/server.py` + routers | FastAPI with CORS, WebSocket real-time progress, job persistence (aiosqlite). Existing routers: core, broll, tts, images, music, etc. |
| **Data Models** | `src/models/` | BRollNeed, BRollPlan, ClipSegment, ScoredVideo, VideoResult, ImageResult, ImageNeed, ScoredImage, ImageGenerationRequest, ContentStyle, VisualStyle, UserPreferences, TranscriptResult, TTSJob. |

### Reuse With Adaptation

| Component | What to Change |
|-----------|---------------|
| **BRollProcessor** (`src/broll_processor.py`, ~1450 lines) | Don't modify. Create new `VideoProductionAgent` orchestrator that reuses same services but follows different pipeline. Copy the DI pattern and parallel processing logic. |
| **AIService prompts** (`src/services/prompts/`) | Add new prompt files: `script_generation.py`, `video_review.py`. Existing prompt versioning + caching infrastructure handles this. |
| **TranscriptionService** | Currently returns sentence-level segments. Need to expose word-level timing from `faster-whisper` (it already provides this data internally). |
| **ClipExtractor** | Reuse Gemini File API upload + analysis flow for Director draft review. Just needs a different prompt. |

---

## Corrections to Original Gemini Plan

| Gemini Suggested | Problem | Use Instead |
|-----------------|---------|-------------|
| ElevenLabs / edge-tts | You already have **5 TTS backends** with voice cloning | `TTSService` (Chatterbox Extended or Qwen3-TTS) |
| Pexels API for B-roll | Pexels is **already integrated** as a pluggable video source, alongside Pixabay and YouTube | Existing multi-source search with competitive analysis |
| Gemini 1.5 Pro | Outdated model | Gemini 3 Flash Preview (already configured) or Gemini 3 Pro |
| Gemini/Imagen for images | You have **3 image gen providers** including FREE Gemini 2.5 Flash | `ImageGenerationService` with Runware/Gemini/Banana |
| `google-genai` SDK fresh install | Already installed and configured | Already in requirements.txt |
| Build music selection from scratch | `MusicService` already generates music via Stable Audio 2.5 | Existing `MusicService` + FFmpeg ducking |
| `moviepy` needs installing | **Already installed** in .venv | Already available |
| Build everything in `/video_agent/` | Ignores existing 1887-line AIService, services architecture | Extend `src/services/prompts/`, add `src/video_agent/` for orchestration only |

---

## Reference Video Analysis

### Videos Analyzed (actual titles from research)

1. **"how to actually become a polymath"** by riskambition (682K views, 8:07)
   - Video essay: voiceover + stock B-roll + Ken Burns effects
   - Dark/moody aesthetic, 3-chapter structure, ~15-20 cuts/min
   - 100% stock footage B-roll - **highest automation potential**

2. **"how to be 'him' asap (no bs guide)"** by Lookin' Fresh (1.5M views, 4:04)
   - Ultra-fast Hormozi-style: word-by-word captions, **30-40 cuts/min**
   - Mix of stock footage + talking head, heavy sound effects
   - Bold animated captions with keyword color highlighting

3. **"how to build a HYBRID athlete physique"** by OnPointFresh (638K views, 6:27)
   - Fitness/educational with outsourced editing (VEditor8)
   - Stock footage + talking head, animated captions, 20-25 cuts/min
   - Professional color grading, balanced pacing

4. **"GREENLAND EXPLAINED: Danish? American?"** by Geopold (395K views, 9:55)
   - Custom animated maps + meme GIFs + infographics
   - Fast-paced meme energy, custom music, integrated text design
   - **Hardest to automate** - requires custom graphics pipeline

### Production Patterns to Replicate

| Element | Pattern | Implementation |
|---------|---------|---------------|
| **Hook** | First 5-10 seconds: provocative statement + dramatic B-roll | Script generator: dedicated `hook` field |
| **Pacing** | **15-40 cuts per minute** (2-4 second clips) | Short `MAX_CLIP_DURATION=4` for video agent |
| **Subtitles** | Word-by-word highlight, bold, centered, 2-3 words at a time, keyword color emphasis | ASS subtitle engine with Hormozi-style grouping |
| **Music** | Continuous ambient/cinematic bed, ducked under narration | Existing `MusicService` + FFmpeg ducking |
| **Transitions** | Mostly hard cuts, occasional cross-dissolve at section breaks | Default to cuts, crossfade at scene boundaries |
| **Narration** | Confident, slightly fast-paced, no hesitation | TTS with lower temperature, Qwen3 instruction: "Speak confidently" |
| **Color grade** | Dark/high-contrast, cinematic look (universal across all 4) | FFmpeg LUT filter application |
| **Visual variety** | Mix of aerial, close-up, wide shots within same topic | Multiple `visual_style` keywords per scene |
| **Sound effects** | Whoosh on transitions, bass drop on key points (Videos 2-3) | SFX library + timeline placement |
| **Text overlays** | Statistics, quotes, key terms appear on screen | Text graphic scene type in script |

### Target Style (Most Automatable)

**Hybrid of Videos 1-3**: Script-driven voiceover + stock B-roll + animated captions + Ken Burns + dark color grade. Duration sweet spot: **4-10 minutes**.

### Difficulty Assessment

**Easiest** (stockpile already does): B-roll search/download, TTS narration, basic subtitles, video concatenation

**Medium** (new code, known solutions): Word-level animated subtitles, Ken Burns on images, audio ducking, color grading via LUT, scene transitions

**Hardest** (Director loop critical): Visual-narration coherence, emotional pacing, music mood matching, the "hook" (first 10 seconds)

---

## Web Research Findings (Critical Updates)

### YouTube "AI Slop" Penalty (IMPORTANT)
YouTube's 2025 algorithm update aggressively de-prioritizes low-effort AI videos. Channels using raw MoneyPrinter-style output get 0 views. Standard Pexels/Pixabay clips are getting channels flagged for **"Reused Content"** and demonetized.

**Mitigations built into this plan:**
- Director-Editor quality loop (no other open-source tool does this)
- Competitive clip analysis (stockpile's existing differentiator)
- AI-generated images for unique visuals (Gemini 2.5 Flash = FREE)
- Color grading + Ken Burns effects make stock footage look custom
- Long-term: integrate AI video generation (Kling/Luma API) for truly unique B-roll

### Use WhisperX Instead of faster-whisper for Subtitles
Standard Whisper gives "fuzzy" word timestamps. **WhisperX** adds Wav2Vec2 forced alignment for millisecond-accurate word-level timing. Essential for Hormozi-style word-by-word captions.
- `pip install whisperx` (replaces faster-whisper for subtitle timing only)

### Use pysubs2 for ASS/SSA Generation
Don't hand-roll ASS format. **pysubs2** (v1.8.0) provides:
- Native ASS/SSA/SRT/WebVTT support
- `load_from_whisper()` for direct Whisper output conversion
- Full style control (fonts, colors, positioning, animations)
- `{\k}` karaoke tags for word-by-word highlighting
- `{\t(0,100,\fscx120\fscy120)}` for pop-in animation effects
- Active maintenance, Python-native

### Director-Editor Loop is Validated
Multiple real implementations exist:
- **VGTeam Framework**: Director/Editor/Painter/Composer roles, iterative revision
- **VideoAgent (HuggingFace)**: Self-conditioning consistency loop
- **VideoDB Director**: Intelligent agent orchestration for video tasks
- All confirm: iterative AI refinement improves semantic alignment, pacing, and logical consistency

### Competitor Analysis: Stockpile Already Wins
| Feature | MoneyPrinterTurbo (22K stars) | ShortGPT (15K stars) | **Stockpile Video Agent** |
|---------|------|---------|--------------------------|
| Script generation | Yes | Yes | Yes |
| TTS | Multiple | ElevenLabs/OpenVoice | **5 backends + voice cloning** |
| B-roll sources | Pexels/Pixabay | Pexels/Pixabay | **Pexels + Pixabay + YouTube** |
| Competitive clip analysis | No | No | **Yes (existing)** |
| Two-pass download | No | No | **Yes (existing)** |
| AI clip scoring | No | No | **Yes (Gemini)** |
| Director quality loop | No | No | **Yes (planned)** |
| AI image generation | No | No | **Yes (3 providers, 1 FREE)** |
| AI music generation | No | No | **Yes (Stable Audio 2.5)** |
| Animated subtitles | Basic SRT | Basic | **ASS/SSA Hormozi-style** |

No open-source alternative has a quality review loop or competitive clip analysis.

### YouTube B-Roll: Fair Use Strategy
Short clips (2-5 seconds) used as transformative B-roll in an entirely different video have strong fair use standing. Every major faceless channel and MoneyPrinterTurbo use this approach. The key is:
- **Cap clip length** at 4-5 seconds (`MAX_BROLL_CLIP_SECONDS=4`)
- **Apply visual transformations** automatically (color grade, Ken Burns, subtitle overlay, crop)
- Content ID rarely triggers on sub-8-second clips with modifications
- See "B-Roll Strategy" section for full implementation details

### Gemini 2.0 Flash/Flash-Lite Retiring March 31, 2026
Upgrade to Gemini 2.5 Flash or Gemini 3 Flash. Stockpile already uses 3 Flash Preview, so this is handled.

### TTS Landscape Update
- **Kokoro** (82M params): Ranked #2 in TTS Arena, 36x real-time on GPU, Apache 2.0, ~$0.02/1M chars. Consider as budget bulk option.
- **Cartesia Sonic 3**: 40ms latency, taking over faceless niche. Cheaper than ElevenLabs.
- **Chatterbox** outperformed ElevenLabs in blind tests (63.8% preference). Your current setup is strong.

---

## Architecture: What Needs to Be Built

Only **4 truly new modules** are needed (music service already exists):

```
src/
  video_agent/                    # NEW - Video production orchestrator
    __init__.py
    agent.py                      # VideoProductionAgent - main orchestrator
    script_generator.py           # Scene-structured script generation
    video_composer.py             # MoviePy/FFmpeg timeline assembly (BIGGEST NEW MODULE)
    subtitle_engine.py            # Word-level animated captions (ASS/SSA)
    director.py                   # Gemini feedback loop
    models.py                     # Scene, Script, Timeline, DraftReview data models
  services/
    prompts/
      script_generation.py        # NEW - Script gen prompt + schema
      video_review.py             # NEW - Director review prompt + schema
  api/
    routers/
      video_agent.py              # NEW - API endpoints for video production
```

### Module Details

#### 1. Script Generator (`script_generator.py`)
**Builds on**: Existing `AIService` with Gemini structured output + prompt versioning

```python
class ScriptGenerator:
    """Generates structured scripts with visual cues from a topic prompt."""

    async def generate(self, topic: str, style: str = "documentary",
                       target_duration_minutes: int = 8) -> Script:
        """
        Uses Gemini to generate a structured script.

        Output Schema:
        {
            "title": str,
            "hook": {
                "voiceover": str,
                "visual_description": str,
                "visual_keywords": [str],
                "sound_effect": str  # "whoosh", "bass_drop", "none"
            },
            "scenes": [
                {
                    "id": int,
                    "duration_est": float,
                    "voiceover": str,
                    "visual_keywords": [str],
                    "visual_style": str,       # "cinematic", "aerial", "close-up", "abstract"
                    "visual_type": str,        # "broll_video" | "generated_image" | "text_graphic"
                    "transition_in": str,      # "cut", "crossfade", "whip"
                    "music_mood": str,         # "tense", "uplifting", "neutral", "dark"
                    "sound_effect": str        # optional SFX marker
                }
            ],
            "metadata": {
                "target_audience": str,
                "tone": str,
                "color_grade": str,  # "dark_cinematic", "warm", "cool", "neutral"
                "estimated_total_duration": float
            }
        }
        """
```

**Key Design Decisions**:
- `visual_type` per scene prevents the "happy dog for economic collapse" problem
- `sound_effect` markers for whoosh/bass_drop placement
- `color_grade` in metadata for consistent LUT application
- `hook` as a dedicated field (not just scene[0]) because the first 5-10 seconds are make-or-break

#### 2. Video Composer (`video_composer.py`) - THE BIGGEST NEW MODULE
**Builds on**: FFmpeg (already installed), MoviePy (already installed in .venv)

```python
class VideoComposer:
    """Assembles final video from audio, visuals, and subtitles."""

    async def compose(self, timeline: Timeline) -> Path:
        """
        Full pipeline:
        1. Normalize all clips to 1920x1080, pad/crop as needed
        2. Lay down narration audio track
        3. Place B-roll clips aligned to narration timestamps
        4. Apply Ken Burns effect to static images (slow zoom/pan)
        5. Apply transitions (cuts default, crossfade at section breaks)
        6. Apply color grade LUT
        7. Overlay ASS subtitles
        8. Mix in background music (auto-ducking under narration)
        9. Add sound effects at markers
        10. Render to output file
        """

    async def compose_draft(self, timeline: Timeline) -> Path:
        """Quick 480p draft for Director review (no effects, no subs)."""

    def apply_ken_burns(self, image_path: Path, duration: float,
                        direction: str = "zoom_in") -> str:
        """FFmpeg zoompan filter for static images."""
        # ffmpeg -loop 1 -i image.jpg -vf "zoompan=z='min(zoom+0.001,1.5)':
        # d=125:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1920x1080"

    def apply_color_grade(self, video_path: Path, lut_name: str) -> Path:
        """Apply LUT via FFmpeg: ffmpeg -i in.mp4 -vf lut3d=dark_cinema.cube out.mp4"""

    def duck_audio(self, music_path: Path, narration_path: Path,
                   duck_level: float = -15) -> Path:
        """FFmpeg sidechaincompress or volume automation for music ducking."""
```

**Rendering Strategy**:
- **Draft render**: 480p, no effects, ultrafast preset → Director review
- **Final render**: 1080p, all effects, CRF 18, medium preset → production
- Render scenes individually with FFmpeg, then concatenate (avoids MoviePy memory issues)
- MoviePy for composition logic only if needed, FFmpeg for all actual encoding

#### 3. Subtitle Engine (`subtitle_engine.py`)
**Builds on**: WhisperX (word-level forced alignment) + pysubs2 (ASS generation)

```python
class SubtitleEngine:
    """Generates word-level animated subtitles (Hormozi/CapCut style)."""

    async def generate_word_timestamps(self, audio_path: Path) -> list[WordTiming]:
        """
        Use WhisperX for millisecond-accurate word-level timing.

        WhisperX > faster-whisper for subtitles because it adds
        Wav2Vec2 forced alignment on top of Whisper, giving much
        tighter word boundaries (~10ms vs ~50ms accuracy).
        """

    def generate_ass_subtitles(self, word_timings: list[WordTiming],
                                style: SubtitleStyle = "hormozi") -> str:
        """
        Generate ASS/SSA subtitle file using pysubs2.

        Styles:
        - "hormozi": Bold Montserrat, centered, 2-3 words at a time,
                     current word highlighted in yellow ({\c&H00FFFF&}),
                     pop-in effect ({\t(0,100,\fscx120\fscy120)}),
                     thick black outline for readability
        - "documentary": Clean, bottom-third, sentence-based, subtle fade
        - "minimal": Small, lower-left, fade in/out

        Uses pysubs2 for:
        - SSAStyle objects with font/color/outline control
        - {\k} karaoke tags for word-by-word highlighting
        - {\t} transform tags for scale/pop animations
        - {\pos} precise positioning

        Why ASS over MoviePy TextClips:
        - 10x faster rendering (no per-frame PIL rasterization)
        - Native FFmpeg: ffmpeg -vf "subtitles=subs.ass:fontsdir=./fonts"
        - Full animation via ASS tags (karaoke, transforms, fades)
        - Industry standard (CapCut, DaVinci Resolve use this internally)
        """

    def burn_subtitles(self, video_path: Path, ass_path: Path) -> Path:
        """Burn ASS subs: ffmpeg -vf subtitles=subs.ass:fontsdir=./fonts"""
```

#### 4. Director - Gemini Feedback Loop (`director.py`)
**Builds on**: Existing Gemini File API upload pattern in `ClipExtractor`

```python
class Director:
    """AI Director that reviews draft cuts and requests refinements."""

    MAX_ITERATIONS = 3

    async def review_draft(self, draft_path: Path, script: Script) -> DraftReview:
        """
        Upload draft to Gemini File API (reuse ClipExtractor._wait_for_file_processing).

        Returns:
        {
            "overall_score": 7,  # 1-10
            "fix_requests": [
                {
                    "scene_id": 3,
                    "issue_type": "visual_mismatch",  # | "pacing" | "audio" | "content_gap"
                    "description": "Shows city at night but narration discusses morning",
                    "suggested_keywords": ["morning cityscape", "sunrise urban"],
                    "priority": "high"  # | "medium" | "low"
                }
            ],
            "approved": false
        }
        """

    async def refine(self, timeline: Timeline, review: DraftReview) -> Timeline:
        """
        Apply fixes:
        - visual_mismatch → Re-search with suggested keywords (use existing video source pipeline)
        - pacing → Adjust transition duration or clip length
        - audio → Adjust music/narration balance
        - content_gap → Generate additional scene
        """
```

**Loop**:
```
Script → Assets → Draft Render (480p) → Director Review
                        ↑                       ↓
                        └── Refine ←── Fix Requests (if !approved)

Max 3 iterations, then force-approve with warnings logged
```

#### 5. Pipeline Orchestrator (`agent.py`)

```python
class VideoProductionAgent:
    """Main orchestrator - topic in, video out."""

    def __init__(self):
        # === REUSE ALL EXISTING SERVICES ===
        self.tts = TTSService()
        self.voice_library = VoiceLibrary()
        self.ai = AIService()
        self.clip_extractor = ClipExtractor()
        self.video_downloader = VideoDownloader()
        self.image_gen = ImageGenerationService()
        self.image_acquisition = ImageAcquisitionService()
        self.music_service = MusicService()           # ALREADY EXISTS!
        self.transcription = TranscriptionService()
        self.file_organizer = FileOrganizer()
        # Video sources (all already exist)
        self.video_sources = [PexelsVideoSource(), PixabayVideoSource(), YouTubeVideoSource()]
        self.image_sources = [PexelsImageSource(), PixabayImageSource(), GoogleImageSource()]

        # === NEW MODULES (only 4) ===
        self.script_gen = ScriptGenerator(self.ai)
        self.composer = VideoComposer()
        self.subtitles = SubtitleEngine(self.transcription)
        self.director = Director(self.ai, self.clip_extractor)

    async def produce(self, topic: str, voice_ref: str | None = None,
                      style: str = "documentary",
                      target_duration: int = 8) -> Path:
        """
        Phase 1: Pre-production
          1. Generate script with Gemini (ScriptGenerator)
          2. Generate narration per-scene with TTS (TTSService)
          3. Merge scene audio into master track
          4. Get word-level timestamps (TranscriptionService/faster-whisper)

        Phase 2: Asset Acquisition (parallel)
          5a. Search & download B-roll per scene (existing video sources + VideoDownloader)
          5b. Generate images for abstract scenes (ImageGenerationService - FREE via Gemini)
          5c. Generate background music (MusicService - Stable Audio 2.5)

        Phase 3: Assembly
          6. Build timeline (match audio timestamps to visual assets)
          7. Generate ASS subtitle file (SubtitleEngine)
          8. Compose draft video at 480p (VideoComposer)

        Phase 4: Director Review (max 3 iterations)
          9. Upload draft to Gemini for review (Director)
          10. If not approved → refine assets/timeline → re-compose → re-review
          11. Repeat until approved or max iterations

        Phase 5: Final Production
          12. Compose final at 1080p with all effects (VideoComposer)
          13. Apply color grade LUT
          14. Burn animated subtitles
          15. Mix music with ducking
          16. Final render → output/
        """
```

---

## Data Models (`models.py`)

```python
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum

class VisualType(Enum):
    BROLL_VIDEO = "broll_video"
    GENERATED_IMAGE = "generated_image"
    TEXT_GRAPHIC = "text_graphic"

class SubtitleStyle(Enum):
    HORMOZI = "hormozi"       # Bold, centered, word-by-word highlight
    DOCUMENTARY = "documentary"  # Clean, bottom-third, sentence-based
    MINIMAL = "minimal"        # Small, lower-left, fade

@dataclass
class HookScript:
    voiceover: str
    visual_description: str
    visual_keywords: list[str]
    sound_effect: str = "none"

@dataclass
class SceneScript:
    id: int
    duration_est: float
    voiceover: str
    visual_keywords: list[str]
    visual_style: str
    visual_type: VisualType
    transition_in: str = "cut"
    music_mood: str = "neutral"
    sound_effect: str = "none"

@dataclass
class Script:
    title: str
    hook: HookScript
    scenes: list[SceneScript]
    metadata: dict

@dataclass
class WordTiming:
    word: str
    start: float
    end: float
    confidence: float = 1.0

@dataclass
class TimelineScene:
    scene_id: int
    audio_path: Path
    audio_start: float
    audio_end: float
    visual_path: Path
    visual_type: VisualType
    transition: str
    word_timings: list[WordTiming] = field(default_factory=list)

@dataclass
class Timeline:
    scenes: list[TimelineScene]
    master_audio: Path
    music_path: Path | None = None
    subtitle_path: Path | None = None
    color_grade: str = "dark_cinematic"
    total_duration: float = 0.0

@dataclass
class FixRequest:
    scene_id: int
    issue_type: str  # "visual_mismatch" | "pacing" | "audio" | "content_gap"
    description: str
    suggested_keywords: list[str] = field(default_factory=list)
    suggested_fix: str = ""
    priority: str = "medium"

@dataclass
class DraftReview:
    overall_score: int
    fix_requests: list[FixRequest]
    approved: bool
    iteration: int = 0
    notes: str = ""
```

---

## Implementation Phases

### Phase 1: MVP Pipeline (Ship something that works)

**Goal**: Topic → Script → Audio → B-roll → Video with subtitles

**New files to create**:
1. `src/video_agent/__init__.py` - Package init
2. `src/video_agent/models.py` - Data models above
3. `src/video_agent/script_generator.py` - Gemini script generation
4. `src/services/prompts/script_generation.py` - Script gen prompt + schema
5. `src/video_agent/subtitle_engine.py` - faster-whisper word timing + ASS generation
6. `src/video_agent/video_composer.py` - FFmpeg-based assembly (Ken Burns, normalization, concatenation, subtitle burn)
7. `src/video_agent/agent.py` - Linear pipeline orchestrator (no Director loop yet)

**New dependencies**: `whisperx` (word-level forced alignment), `pysubs2` (ASS/SSA generation). moviepy already installed.

**What this produces**: A watchable video with narration, B-roll from 3 sources (Pexels/Pixabay/YouTube), generated images for abstract scenes, background music (Stable Audio), and Hormozi-style subtitles. No Director review loop yet.

### Phase 2: Director + Polish

**Goal**: Add AI review loop, color grading, sound effects

**Files to create/modify**:
1. `src/video_agent/director.py` - Gemini feedback loop
2. `src/services/prompts/video_review.py` - Director review prompt + schema
3. Update `video_composer.py` - Add color grade LUT, SFX placement, transition effects
4. Update `agent.py` - Add Director review loop (max 3 iterations)

**What this adds**: Videos get iteratively refined. Mismatched B-roll gets replaced. Pacing issues get fixed. Color grading makes it look cinematic.

### Phase 3: Advanced Features

**Goal**: Multi-style support, web UI, YouTube integration

1. Style presets (documentary, motivational, educational, Hormozi-fast)
2. Batch video generation (queue of topics with checkpoint/resume)
3. Web UI for video production (add `src/api/routers/video_agent.py`, use existing WebSocket progress)
4. A/B script testing (competitive analysis pattern: generate 3 scripts, pick best)
5. Thumbnail generation using existing `ImageGenerationService` (Gemini Flash = FREE)
6. YouTube upload integration
7. SFX library (whoosh, bass drop, transition sounds)

---

## Technical Decisions

### Video Composition: FFmpeg-First

**Use FFmpeg directly** (not MoviePy) for all rendering. Reasons:
- MoviePy has memory issues with videos >5 minutes
- FFmpeg is already a dependency and battle-tested in stockpile
- All operations map to FFmpeg filters:
  - Ken Burns: `zoompan` filter
  - Subtitles: `ass` filter
  - Color grade: `lut3d` filter
  - Audio ducking: `sidechaincompress` or volume automation
  - Concatenation: concat demuxer
  - Transitions: `xfade` filter
- Use MoviePy only if a specific composition need can't be done with FFmpeg filter graphs

### Subtitle Stack: WhisperX + pysubs2 + ASS/SSA

- **Timing**: WhisperX with Wav2Vec2 forced alignment (~10ms word accuracy vs ~50ms from standard Whisper)
- **Generation**: pysubs2 library for programmatic ASS/SSA creation (karaoke tags, transforms, styles)
- **Rendering**: FFmpeg burn-in: `ffmpeg -vf "subtitles=subs.ass:fontsdir=./fonts"`
- **Why not SRT**: No styling, no animation
- **Why not MoviePy TextClips**: Renders each frame with PIL - extremely slow, memory hog
- **Why ASS**: Full animation, native FFmpeg support, industry standard (CapCut uses this internally)

### TTS Strategy

**Recommended**: Chatterbox Extended (multi-candidate + Whisper validation)
**Alternative**: Qwen3-TTS with `instruction="Speak confidently and clearly"` + preset speakers

**Generate per-scene audio** (not one long narration):
- Allows re-generating individual scenes on Director feedback
- Easier timestamp alignment
- Merge scene WAVs into master audio at composition time

### B-Roll Strategy

**All 3 sources, used together**:
1. **Pexels** (CC0, free, high quality, 200 req/hr) - safe default for generic visuals
2. **Pixabay** (CC0, free, 5000 req/hr) - high-volume supplement
3. **YouTube** (yt-dlp) - best variety, niche topics, highest visual quality

**For abstract concepts**: Use `ImageGenerationService` with Gemini 2.5 Flash (FREE, 500/day)

**Clip duration**: 2-4 seconds for this style (not stockpile's default 4-15s)

#### YouTube B-Roll: Fair Use Approach

YouTube clips are the highest quality and most diverse B-roll source available. To use them safely:

**`MAX_BROLL_CLIP_SECONDS = 4`** (configurable) - Cap all YouTube-sourced clips at 4-5 seconds. Short clips used as illustrative B-roll in a completely different video have a strong fair use position:
- **Purpose**: Transformative (illustrative, not reposting)
- **Amount**: Tiny fraction of source (4s from a 10+ min video)
- **Market effect**: Zero (not competing with original)

**Mandatory transformations on all YouTube clips** (applied automatically by VideoComposer):
- Color grade LUT (changes the visual identity)
- Ken Burns zoom/pan (adds unique movement)
- Subtitle overlay (further transforms the frame)
- Resolution/crop adjustment (reframed, not 1:1)

These transformations are not just legal protection - they're what makes the output look professional instead of stitched-together stock footage. Content ID rarely triggers on clips under ~8 seconds with visual modifications applied.

**Configuration**:
```bash
YOUTUBE_BROLL_MAX_SECONDS=4          # Max clip length from YouTube sources
BROLL_TRANSFORM_REQUIRED=true        # Force color grade + Ken Burns on all clips
BROLL_SOURCE_PRIORITY=pexels,pixabay,youtube  # Search order
```

**"Reused Content" defense** (applies to ALL sources, not just YouTube):
- Color grading (LUT) makes any stock footage look custom
- Ken Burns effects add unique movement to every clip
- AI-generated images for visual variety
- Director loop catches visual repetition across scenes
- Long-term: AI video generation (Kling/Luma) for truly unique B-roll

### Gemini Model Selection

| Task | Model | Why |
|------|-------|-----|
| Script generation | Gemini 3 Pro | Creative quality, structured output |
| B-roll evaluation | Gemini 3 Flash (existing) | Already working, fast, cheap |
| Director video review | Gemini 3 Pro | Visual understanding + reasoning |
| Image prompt generation | Gemini 3 Flash (existing) | Already working in AIService |

---

## Cost Estimate Per Video (8-minute video)

| Component | Provider | Cost |
|-----------|----------|------|
| Script generation | Gemini 3 Pro | ~$0.02 |
| TTS narration | Chatterbox Extended (RunPod) | ~$0.05-0.15 |
| B-roll search + evaluation | Gemini 3 Flash | ~$0.05 |
| Fallback image generation | Gemini 2.5 Flash | **FREE** |
| Background music | Stable Audio 2.5 (Replicate) | ~$0.05 |
| Director review x3 | Gemini 3 Pro | ~$0.06 |
| Word-level timestamps | faster-whisper (local) | **FREE** |
| **Total** | | **~$0.25-0.35** |

---

## Example Usage

```python
# Simple
agent = VideoProductionAgent()
video_path = await agent.produce("The History of Coffee")

# With voice cloning and style
video_path = await agent.produce(
    topic="How AI is Changing Medicine",
    voice_ref="/path/to/narrator_sample.wav",
    style="documentary",
    target_duration=8
)

# Ultra-fast Hormozi style
video_path = await agent.produce(
    topic="5 Habits That Changed My Life",
    style="hormozi",
    target_duration=4
)
```

```bash
# CLI
python -m src.video_agent "The History of Coffee" --style documentary --duration 8
python -m src.video_agent "5 Habits That Changed My Life" --style hormozi --duration 4
```

---

## Risk Assessment

| Risk | Mitigation |
|------|-----------|
| FFmpeg filter complexity | Build incrementally: concat first, then add Ken Burns, then LUT, then subtitles |
| Gemini Director gets stuck in loop | Max 3 iterations, force-approve with warnings, log all feedback |
| B-roll doesn't match narration | Director catches mismatches, suggests better keywords. Multi-source fallback. |
| TTS quality inconsistency | Chatterbox Extended multi-candidate + Whisper validation auto-rejects bad audio |
| YouTube rate limiting | Already handled by existing retry/backoff. Pexels/Pixabay as primary sources. |
| Background music copyright | Stable Audio generates royalty-free. No pre-existing tracks. |
| Long render times | Draft at 480p ultrafast for iteration, 1080p only for final approved version |
| Memory issues on long videos | Render scenes individually, concatenate with FFmpeg concat demuxer |

---

## File-by-File Implementation Checklist

### Phase 1 (MVP) - ~7 new files

- [ ] `src/video_agent/__init__.py`
- [ ] `src/video_agent/models.py` - All data classes
- [ ] `src/video_agent/script_generator.py` - `ScriptGenerator.generate(topic) -> Script`
- [ ] `src/services/prompts/script_generation.py` - Prompt template + JSON schema
- [ ] `src/video_agent/subtitle_engine.py` - Word timestamps + ASS generation + burn
- [ ] `src/video_agent/video_composer.py` - FFmpeg assembly (normalize, concat, Ken Burns, subs, music mix)
- [ ] `src/video_agent/agent.py` - `VideoProductionAgent.produce()` linear pipeline

### Phase 2 (Director + Polish) - ~3 new files

- [ ] `src/video_agent/director.py` - Gemini review + refine loop
- [ ] `src/services/prompts/video_review.py` - Director review prompt + schema
- [ ] Update `agent.py` - Integrate Director loop
- [ ] Update `video_composer.py` - Color grade LUT, SFX markers

### Phase 3 (Advanced) - extend existing

- [ ] `src/api/routers/video_agent.py` - Web API endpoints
- [ ] Style presets configuration
- [ ] Batch production with existing checkpoint/resume
- [ ] YouTube upload integration
